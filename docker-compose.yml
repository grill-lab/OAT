version: "3.9"  # optional since v1.27.0
services:

  builder:
    container_name: bob
    build:
      context: ./
      dockerfile: builder/Dockerfile
    volumes:
      - ./builder:/source
      - ./shared:/shared

  downloader:
    container_name: downloader
    profiles: ["downloader"]
    build:
      context: ./
      dockerfile: downloader/Dockerfile
    volumes:
      - ./shared:/shared
      - ./downloader:/source
      # services which have downloads.toml at the moment
      - ./offline:/offline
      - ./llm_functionalities:/llm_functionalities
      - ./neural_functionalities:/neural_functionalities
      - ./functionalities:/functionalities
      - ./training:/training
    environment:
      - CONTAINER_NAME=downloader
    depends_on:
      - builder
      - oat_common
    networks:
      - internal
      - external

  local_client:
    container_name: local_client
    build:
      context: ./
      dockerfile: local_client/Dockerfile
    ports:
      - "9000:8000"
    volumes:
      - ./local_client:/source
      - ./shared:/shared
    environment:
      - CONTAINER_NAME=local_client
      - DISTRIBUTOR_URL=http://orchestrator:8000
    networks:
      - internal
      - external
    depends_on:
      - builder
      - oat_common

  oat_common:
    container_name: oat_common
    build:
      context: ./
      dockerfile: oat_common/Dockerfile
    image: oat_common

  orchestrator:
    container_name: orchestrator
    build:
      context: ./
      dockerfile: orchestrator/Dockerfile
    volumes:
      - ./orchestrator:/source
      - ./shared:/shared
    environment:
      - CONTAINER_NAME=orchestrator
      - FUNCTIONALITIES_URL=functionalities:8000
      - EXTERNAL_FUNCTIONALITIES_URL=external_functionalities:8000
      - NEURAL_FUNCTIONALITIES_URL=neural_functionalities:8000
      - LLM_FUNCTIONALITIES_URL=llm_functionalities:8000
    networks:
      - internal
    depends_on:
      - builder
      - oat_common
      - functionalities
      - external_functionalities
      - neural_functionalities

  functionalities:
    container_name: functionalities
    build:
      context: ./
      dockerfile: functionalities/Dockerfile
    volumes:
      - ./functionalities:/source
      - ./shared:/shared
    environment:
      - CONTAINER_NAME=functionalities
      - EXTERNAL_FUNCTIONALITIES_URL=external_functionalities:8000
      - FUNCTIONALITIES_URL=functionalities:8000
      - NEURAL_FUNCTIONALITIES_URL=neural_functionalities:8000
      - LLM_FUNCTIONALITIES_URL=llm_functionalities:8000
    networks:
      - internal
      - external
    depends_on:
      - builder
      - oat_common

  external_functionalities:
    container_name: external_functionalities
    build:
      context: ./
      dockerfile: external_functionalities/Dockerfile
    volumes:
      - ./external_functionalities:/source
      - ./shared:/shared
    environment:
      - CONTAINER_NAME=external_functionalities
      - DATABASE_URL=http://dynamodb-local:8000
      - FUNCTIONALITIES_URL=functionalities:8000
    networks:
      - internal
      - external
    depends_on:
      - builder
      - oat_common
      - dynamodb-local

  neural_functionalities:
    container_name: neural_functionalities
    build:
      context: ./
      dockerfile: neural_functionalities/Dockerfile
    volumes:
      - ./neural_functionalities:/source
      - ./shared:/shared
    environment:
      - CONTAINER_NAME=neural_functionalities
      - NEURAL_FUNCTIONALITIES_URL=neural_functionalities:8000
      - FUNCTIONALITIES_URL=functionalities:8000
      - EXTERNAL_FUNCTIONALITIES_URL=external_functionalities:8000
    networks:
      - internal
      - external
    depends_on:
      builder:
        condition: service_started
      oat_common:
        condition: service_started


  dynamodb-local:
    container_name: dynamodb-local
    image: "amazon/dynamodb-local:latest"
    ports:
      - "8888:8000"
    volumes:
      - "./shared/file_system/dynamodb:/home/dynamodblocal/data"
    working_dir: /home/dynamodblocal
    command: "-jar DynamoDBLocal.jar -sharedDb -dbPath ./data"
    user: root
    networks:
      - internal
      - external

  dashboard:
    container_name: dashboard
    profiles: ["dashboard"]
    build:
      context: ./
      dockerfile: dashboard/Dockerfile
    ports:
      - "7500:7500"
    volumes:
      - ./dashboard:/source
      - ./shared:/shared
    environment:
      - CONTAINER_NAME=dashboard
      - DB_ENVIRONMENT=Undefined
      - DATABASE_URL=http://dynamodb-local:8000
    networks:
      - internal
      - external
    depends_on:
      - builder
      - dynamodb-local
      - oat_common

  tester:
    container_name: tester
    profiles: ["tester"]
    build:
      context: ./
      dockerfile: tester/Dockerfile
    volumes:
      - ./tester:/source
      - ./shared:/shared
    environment:
      - CONTAINER_NAME=tester
      - FUNCTIONALITIES_URL=functionalities:8000
      - EXTERNAL_FUNCTIONALITIES_URL=external_functionalities:8000
      - NEURAL_FUNCTIONALITIES_URL=neural_functionalities:8000
      - DISTRIBUTOR_URL=http://orchestrator:8000
    networks:
      - external
      - internal
    depends_on:
      - builder
      - oat_common

  offline:
    container_name: offline
    profiles: ["offline"]
    build:
      context: ./
      dockerfile: offline/Dockerfile
    volumes:
      - ./offline:/source
      - ./shared:/shared
      - ./testing/integration_tests:/integration_tests
    environment:
      - CONTAINER_NAME=offline
      - FUNCTIONALITIES_URL=functionalities:8000
    networks:
      - internal
      - external
    depends_on:
      - oat_common
      - dynamodb-local
    deploy:
      resources:
        reservations:
          devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]


  sphinx_docs:
    container_name: sphinx_docs
    profiles: ["sphinx_docs"]
    build:
      context: ./
      dockerfile: sphinx_docs/Dockerfile
    volumes:
      # this is a bit different from the other containers in that it mounts
      # the entire OAT folder at /source. this allows Sphinx to have
      # access to the entire codebase instead of a subfolder
      - ./:/source
      # shared is accessible through the /source mount, but to make the paths
      # work as expected mount it separately too
      - ./shared:/shared
    environment:
      - CONTAINER_NAME=sphinx_docs
    depends_on:
      - builder

  training:
    container_name: training
    profiles: ["training"]
    build:
      context: ./
      dockerfile: training/Dockerfile
    volumes:
      - ./training:/source
      - ./shared:/shared
    environment:
      - CONTAINER_NAME=training
    networks:
      - internal
      - external
    depends_on:
      - builder
      - oat_common
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]

  llm_functionalities:
    container_name: llm_functionalities
    build:
      context: ./
      dockerfile: llm_functionalities/Dockerfile
    volumes:
      - ./llm_functionalities:/source
      - ./shared:/shared
    environment:
      - CONTAINER_NAME=llm_functionalities
      - NEURAL_FUNCTIONALITIES_URL=llm_functionalities:8000
      - FUNCTIONALITIES_URL=functionalities:8000
      - EXTERNAL_FUNCTIONALITIES_URL=external_functionalities:8000
      # the location of the TGI endpoint (note that it's using the internal
      # Docker network, so it's port 80 rather than 8081)
      - INFERENCE_ENDPOINT_URL=tgi:80
      # these values can be used to control how long this service waits
      # for the TGI endpoint to become available. this might take a significant
      # amount of time in some cases, e.g. if it has to download a large model
      # Number of retries (default 10)
      - TGI_CONNECTION_RETRY_LIMIT=${TGI_CONNECTION_RETRY_LIMIT:-10}
      # delay between retries in seconds
      - TGI_CONNECTION_RETRY_DELAY=${TGI_CONNECTION_RETRY_DELAY:-10}
    networks:
      - internal
      - external
    depends_on:
      builder:
        condition: service_started
      tgi:
        condition: service_started

  tgi:
    platform: linux/x86_64
    container_name: tgi
    build:
      context: ./
      dockerfile: hg_tgi/Dockerfile
    ports:
      # this isn't needed for llm_functionalities (it uses the internal network),
      # but might be useful to be able to submit requests to the instance from
      # external scripts for testing/debugging
      - "8081:80"
    volumes:
      # the container downloads weights and other files to this path
      - ./shared/file_system/tgi:/data
    environment:
      # setting the value of MODEL_ID is equivalent to passing "--model-id" parameter
      # to the TGI launcher
      - MODEL_ID=${MODEL_ID:-google/flan-t5-large}
      # any other TGI launcher parameters can be set in this env var, e.g.:
      # TGI_PARAMS="--param1 param1_value --param2 param2_value" docker compose up
      - TGI_PARAMS=${TGI_PARAMS:-}
    networks:
      - internal
      - external
    # larger sharded models will need this increased from the default (usually 64MB)
    shm_size: ${SHM_SIZE:-2gb}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              # set the number of GPUs available to the container, default to 1
              count: ${GPU_COUNT:-1}
              capabilities: [ gpu ]

networks:
  internal:
    internal: true
  external:
    internal: false

